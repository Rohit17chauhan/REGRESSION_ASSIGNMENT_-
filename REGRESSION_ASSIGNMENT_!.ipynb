{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4fae59",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee535cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Simple Linear Regression and Multiple Linear Regression are both types of linear regression models used in statistics\\nand machine learning to model the relationship between a dependent variable and one or more independent variables.\\n\\nSimple Linear Regression\\nDefinition: Simple linear regression models the relationship between a dependent variable and a single independent variable.\\nThe model assumes a linear relationship between the two, represented by the equation:\\n\\ny=Œ≤0+B1X\\nExample: Suppose you want to predict a person's height based on their age. Here, the height is the dependent variable, \\nand the age is the independent variable.\\n\\nMultiple Linear Regression\\nDefinition: Multiple linear regression models the relationship between a dependent variable and two or more independent\\nvariables. The model assumes a linear relationship between the dependent variable and each of the independent variables, \\nrepresented by the equation:\\ny=Œ≤0+B1X+B2x+....+BnXn\\n\\nExample: Suppose you want to predict a person's weight based on their height, age, and gender. Here, weight is the dependent\\nvariable, and height, age, and gender are independent variables. \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans1=\"\"\"Simple Linear Regression and Multiple Linear Regression are both types of linear regression models used in statistics\n",
    "and machine learning to model the relationship between a dependent variable and one or more independent variables.\n",
    "\n",
    "Simple Linear Regression\n",
    "Definition: Simple linear regression models the relationship between a dependent variable and a single independent variable.\n",
    "The model assumes a linear relationship between the two, represented by the equation:\n",
    "\n",
    "y=Œ≤0+B1X\n",
    "Example: Suppose you want to predict a person's height based on their age. Here, the height is the dependent variable, \n",
    "and the age is the independent variable.\n",
    "\n",
    "Multiple Linear Regression\n",
    "Definition: Multiple linear regression models the relationship between a dependent variable and two or more independent\n",
    "variables. The model assumes a linear relationship between the dependent variable and each of the independent variables, \n",
    "represented by the equation:\n",
    "y=Œ≤0+B1X+B2x+....+BnXn\n",
    "\n",
    "Example: Suppose you want to predict a person's weight based on their height, age, and gender. Here, weight is the dependent\n",
    "variable, and height, age, and gender are independent variables. \n",
    "\"\"\"\n",
    "Ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b724af",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcf9a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear regression models rely on several key assumptions to produce valid, reliable results. These assumptions must \\nbe met to ensure that the model accurately represents the data and that the statistical inferences made from the model are \\nvalid.\\n\\nAssumptions of Linear Regression\\nLinearity:\\n\\nThe relationship between the dependent variable and the independent variables should be linear. This means that changes in\\nthe independent variables should correspond to proportional changes in the dependent variable.\\nIndependence:\\n\\nObservations should be independent of each other. This means that the value of the dependent variable for one observation \\nshould not be related to the value of the dependent variable for another observation.\\n\\nChecking the Assumptions in a Given Dataset\\nLinearity:\\n\\nCheck: Plot the dependent variable against each independent variable (scatter plots) or create a residual plot \\n(plot of residuals vs. predicted values).\\nWhat to Look For: A linear pattern in scatter plots indicates linearity. In a residual plot, there should be no clear pattern \\nor curvature.\\n\\nIndependence:\\n\\nCheck: Examine the study design to ensure observations are independent. For time series data, a Durbin-Watson test can be used \\nto check for autocorrelation.\\nWhat to Look For: Independent observations will have no systematic pattern in the data collection process, and the \\nDurbin-Watson statistic should be close to 2.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans2=\"\"\"Linear regression models rely on several key assumptions to produce valid, reliable results. These assumptions must \n",
    "be met to ensure that the model accurately represents the data and that the statistical inferences made from the model are \n",
    "valid.\n",
    "\n",
    "Assumptions of Linear Regression\n",
    "Linearity:\n",
    "\n",
    "The relationship between the dependent variable and the independent variables should be linear. This means that changes in\n",
    "the independent variables should correspond to proportional changes in the dependent variable.\n",
    "Independence:\n",
    "\n",
    "Observations should be independent of each other. This means that the value of the dependent variable for one observation \n",
    "should not be related to the value of the dependent variable for another observation.\n",
    "\n",
    "Checking the Assumptions in a Given Dataset\n",
    "Linearity:\n",
    "\n",
    "Check: Plot the dependent variable against each independent variable (scatter plots) or create a residual plot \n",
    "(plot of residuals vs. predicted values).\n",
    "What to Look For: A linear pattern in scatter plots indicates linearity. In a residual plot, there should be no clear pattern \n",
    "or curvature.\n",
    "\n",
    "Independence:\n",
    "\n",
    "Check: Examine the study design to ensure observations are independent. For time series data, a Durbin-Watson test can be used \n",
    "to check for autocorrelation.\n",
    "What to Look For: Independent observations will have no systematic pattern in the data collection process, and the \n",
    "Durbin-Watson statistic should be close to 2.\n",
    "\"\"\"\n",
    "Ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0994c",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b065da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In a linear regression model, the slope and intercept are key components that define the relationship between the \\nindependent variable(s) and the dependent variable. Here's how you interpret them:\\n\\nIntercept (B0):\\nDefinition: The intercept is the value of the dependent variable (Y) when all independent variables (X) are equal to zero.\\nIt represents the baseline level of Y in the absence of any influence from the independent variables.\\nInterpretation: The intercept tells you where the regression line crosses the y-axis. It's the predicted value of ùëåwhen X=0.\\n\\n\\nSLOPE(B1):\\nDefinition: The slope represents the change in the dependent variable (Y) for a one-unit increase in the independent\\nvariable (X). It measures the strength and direction of the relationship between X and Y.\\nInterpretation: The slope indicates how much Y is expected to change as X increases by one unit. A positive slope suggests \\nthat as X increases, Y also increases. A negative slope suggests that as X increases, Y decreases.\\n\\nReal-World Scenario Example\\nImagine you are a data analyst working for a retail company, and you want to predict the monthly sales revenue (in thousands of dollars) based on the amount spent on advertising (in thousands of dollars).\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans3=\"\"\"In a linear regression model, the slope and intercept are key components that define the relationship between the \n",
    "independent variable(s) and the dependent variable. Here's how you interpret them:\n",
    "\n",
    "Intercept (B0):\n",
    "Definition: The intercept is the value of the dependent variable (Y) when all independent variables (X) are equal to zero.\n",
    "It represents the baseline level of Y in the absence of any influence from the independent variables.\n",
    "Interpretation: The intercept tells you where the regression line crosses the y-axis. It's the predicted value of ùëåwhen X=0.\n",
    "\n",
    "\n",
    "SLOPE(B1):\n",
    "Definition: The slope represents the change in the dependent variable (Y) for a one-unit increase in the independent\n",
    "variable (X). It measures the strength and direction of the relationship between X and Y.\n",
    "Interpretation: The slope indicates how much Y is expected to change as X increases by one unit. A positive slope suggests \n",
    "that as X increases, Y also increases. A negative slope suggests that as X increases, Y decreases.\n",
    "\n",
    "Real-World Scenario Example\n",
    "Imagine you are a data analyst working for a retail company, and you want to predict the monthly sales revenue (in thousands of dollars) based on the amount spent on advertising (in thousands of dollars).\n",
    "\"\"\"\n",
    "Ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a255487",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069b7881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradient descent is an optimization algorithm used to minimize the cost function (or loss function) in machine learning \\nand other optimization problems. It‚Äôs a key component in training models, particularly in supervised learning, where the goal \\nis to find the optimal parameters (e.g., weights in a neural network) that minimize the error between the predicted output and\\nthe actual output.\\n\\nConcept of Gradient Descent\\nCost Function:\\n\\nThe cost function measures how well a machine learning model is performing. In linear regression, for example, the cost \\nfunction is often the Mean Squared Error (MSE), which calculates the average squared difference between the predicted values\\nand the actual values.\\nThe goal of gradient descent is to find the set of parameters (e.g., coefficients in linear regression) that minimize the cost \\nfunction.\\nGradient:\\n\\nThe gradient is a vector of partial derivatives of the cost function with respect to each parameter. It points in the direction\\nof the steepest increase in the cost function. In other words, it indicates how much the cost function will change if you \\nchange the parameters slightly.\\nIn gradient descent, you move in the opposite direction of the gradient (i.e., downhill) because you want to minimize the\\ncost function.\\nDescent:\\n\\nGradient descent updates the parameters iteratively, moving them in small steps toward the direction that reduces the cost \\nfunction.\\nThe size of the steps is determined by the learning rate, a hyperparameter that you set before running the algorithm.\\nA learning rate that is too large can cause the algorithm to overshoot the minimum, while a learning rate that is too small\\ncan make the process very slow.\\n\\nHow Gradient Descent Works\\nInitialize Parameters:\\n\\nStart with an initial guess for the parameters (often initialized randomly).\\nCompute Gradient:\\n\\nCalculate the gradient of the cost function with respect to each parameter using the current parameter values.\\nUpdate Parameters:\\n\\nAdjust the parameters by moving them in the direction that decreases the cost function\\nRepeat:\\n\\nRepeat the process of computing the gradient and updating the parameters until the algorithm converges, i.e., when the change \\nin the cost function is sufficiently small or the maximum number of iterations is reached.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans4=\"\"\"Gradient descent is an optimization algorithm used to minimize the cost function (or loss function) in machine learning \n",
    "and other optimization problems. It‚Äôs a key component in training models, particularly in supervised learning, where the goal \n",
    "is to find the optimal parameters (e.g., weights in a neural network) that minimize the error between the predicted output and\n",
    "the actual output.\n",
    "\n",
    "Concept of Gradient Descent\n",
    "Cost Function:\n",
    "\n",
    "The cost function measures how well a machine learning model is performing. In linear regression, for example, the cost \n",
    "function is often the Mean Squared Error (MSE), which calculates the average squared difference between the predicted values\n",
    "and the actual values.\n",
    "The goal of gradient descent is to find the set of parameters (e.g., coefficients in linear regression) that minimize the cost \n",
    "function.\n",
    "Gradient:\n",
    "\n",
    "The gradient is a vector of partial derivatives of the cost function with respect to each parameter. It points in the direction\n",
    "of the steepest increase in the cost function. In other words, it indicates how much the cost function will change if you \n",
    "change the parameters slightly.\n",
    "In gradient descent, you move in the opposite direction of the gradient (i.e., downhill) because you want to minimize the\n",
    "cost function.\n",
    "Descent:\n",
    "\n",
    "Gradient descent updates the parameters iteratively, moving them in small steps toward the direction that reduces the cost \n",
    "function.\n",
    "The size of the steps is determined by the learning rate, a hyperparameter that you set before running the algorithm.\n",
    "A learning rate that is too large can cause the algorithm to overshoot the minimum, while a learning rate that is too small\n",
    "can make the process very slow.\n",
    "\n",
    "How Gradient Descent Works\n",
    "Initialize Parameters:\n",
    "\n",
    "Start with an initial guess for the parameters (often initialized randomly).\n",
    "Compute Gradient:\n",
    "\n",
    "Calculate the gradient of the cost function with respect to each parameter using the current parameter values.\n",
    "Update Parameters:\n",
    "\n",
    "Adjust the parameters by moving them in the direction that decreases the cost function\n",
    "Repeat:\n",
    "\n",
    "Repeat the process of computing the gradient and updating the parameters until the algorithm converges, i.e., when the change \n",
    "in the cost function is sufficiently small or the maximum number of iterations is reached.\n",
    "\"\"\"\n",
    "Ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcabb6d",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569e4b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A multiple linear regression model is an extension of simple linear regression that involves predicting a dependent \\n\\nvariable based on more than one independent variable. In this model, each independent variable contributes to the prediction \\nof the dependent variable, allowing for a more complex and realistic representation of real-world relationships.\\n\\nIn multiple linear regression, the relationship between the dependent variable and the independent variables is assumed to be\\nlinear. The model combines the effects of all the independent variables, each with its own coefficient, to estimate the\\ndependent variable. These coefficients represent the change in the dependent variable for a one-unit change in the \\ncorresponding independent variable, assuming all other variables are held constant.\\n\\nThe primary difference between multiple linear regression and simple linear regression is the number of independent \\nvariables involved. Simple linear regression uses only one independent variable to predict the dependent variable, making it \\na straightforward model that captures a basic linear relationship. In contrast, multiple linear regression incorporates two \\nor more independent variables, enabling the model to account for the combined influence of multiple factors, which is often \\nnecessary for more accurate predictions in complex scenarios. This added complexity allows multiple linear regression to model\\nmore intricate relationships, but it also requires careful consideration of issues like multicollinearity, where independent\\nvariables may be correlated with each other, potentially affecting the reliability of the model.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans5=\"\"\"A multiple linear regression model is an extension of simple linear regression that involves predicting a dependent \n",
    "\n",
    "variable based on more than one independent variable. In this model, each independent variable contributes to the prediction \n",
    "of the dependent variable, allowing for a more complex and realistic representation of real-world relationships.\n",
    "\n",
    "In multiple linear regression, the relationship between the dependent variable and the independent variables is assumed to be\n",
    "linear. The model combines the effects of all the independent variables, each with its own coefficient, to estimate the\n",
    "dependent variable. These coefficients represent the change in the dependent variable for a one-unit change in the \n",
    "corresponding independent variable, assuming all other variables are held constant.\n",
    "\n",
    "The primary difference between multiple linear regression and simple linear regression is the number of independent \n",
    "variables involved. Simple linear regression uses only one independent variable to predict the dependent variable, making it \n",
    "a straightforward model that captures a basic linear relationship. In contrast, multiple linear regression incorporates two \n",
    "or more independent variables, enabling the model to account for the combined influence of multiple factors, which is often \n",
    "necessary for more accurate predictions in complex scenarios. This added complexity allows multiple linear regression to model\n",
    "more intricate relationships, but it also requires careful consideration of issues like multicollinearity, where independent\n",
    "variables may be correlated with each other, potentially affecting the reliability of the model.\n",
    "\"\"\"\n",
    "Ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29854e0b",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1670f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Multicollinearity is a statistical concept that occurs when two or more independent variables in a regression model are correlated. It can negatively impact a model's ability to make accurate predictions and can lead to less reliable statistical inferences. \\nHere are some ways to detect and address multicollinearity: \\nLook for correlation: A significant correlation between independent variables is often the first sign of multicollinearity. \\nRemove highly correlated variables: You can remove independent variables that are highly correlated. \\nUse a dimensionality reduction technique: You can use a technique like principal component analysis (PCA) to combine highly correlated features. \\nUse a regression algorithm that can handle multicollinearity: You can try using a regression algorithm like ridge regression.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans6=\"\"\"Multicollinearity is a statistical concept that occurs when two or more independent variables in a regression model are correlated. It can negatively impact a model's ability to make accurate predictions and can lead to less reliable statistical inferences. \n",
    "Here are some ways to detect and address multicollinearity: \n",
    "Look for correlation: A significant correlation between independent variables is often the first sign of multicollinearity. \n",
    "Remove highly correlated variables: You can remove independent variables that are highly correlated. \n",
    "Use a dimensionality reduction technique: You can use a technique like principal component analysis (PCA) to combine highly correlated features. \n",
    "Use a regression algorithm that can handle multicollinearity: You can try using a regression algorithm like ridge regression.\"\"\"\n",
    "Ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db13a9",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37d5e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polynomial regression is a type of regression analysis that models the relationship between the independent\\nvariable(s) and the dependent variable as an nth-degree polynomial. Unlike linear regression, which assumes a linear \\nrelationship between the independent and dependent variables, polynomial regression can model more complex, non-linear\\nrelationships by introducing polynomial terms of the independent variable.\\n\\nHow Polynomial Regression Differs from Linear Regression:\\nComplexity:\\n\\nLinear Regression: Simpler and easier to interpret since it only involves a single slope and intercept.\\nPolynomial Regression: More complex due to the additional polynomial terms, which can lead to overfitting if the degree of \\nthe polynomial is too high.\\nFlexibility:\\n\\nLinear Regression: Limited in its ability to model non-linear relationships. If the relationship between X and Y is not linear,\\na linear model might not perform well.\\nPolynomial Regression: More flexible as it can fit a wider range of data patterns. It can curve and bend to better match the \\ndata, making it suitable for scenarios where the relationship is non-linear.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans7=\"\"\"Polynomial regression is a type of regression analysis that models the relationship between the independent\n",
    "variable(s) and the dependent variable as an nth-degree polynomial. Unlike linear regression, which assumes a linear \n",
    "relationship between the independent and dependent variables, polynomial regression can model more complex, non-linear\n",
    "relationships by introducing polynomial terms of the independent variable.\n",
    "\n",
    "How Polynomial Regression Differs from Linear Regression:\n",
    "Complexity:\n",
    "\n",
    "Linear Regression: Simpler and easier to interpret since it only involves a single slope and intercept.\n",
    "Polynomial Regression: More complex due to the additional polynomial terms, which can lead to overfitting if the degree of \n",
    "the polynomial is too high.\n",
    "Flexibility:\n",
    "\n",
    "Linear Regression: Limited in its ability to model non-linear relationships. If the relationship between X and Y is not linear,\n",
    "a linear model might not perform well.\n",
    "Polynomial Regression: More flexible as it can fit a wider range of data patterns. It can curve and bend to better match the \n",
    "data, making it suitable for scenarios where the relationship is non-linear.\n",
    "\"\"\"\n",
    "Ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16f93f",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e56f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advantages of Polynomial Regression Compared to Linear Regression\\nAbility to Model Non-Linear Relationships:\\n\\nPolynomial Regression: Can capture complex, non-linear relationships between the independent and dependent variables by\\nincorporating higher-degree polynomial terms. This makes it suitable for datasets where the relationship is not well-represented\\nby a straight line.\\nLinear Regression: Limited to modeling linear relationships, which may not accurately capture the underlying patterns in \\nmore complex datasets.\\nFlexibility:\\n\\nPolynomial Regression: Offers greater flexibility as it can fit curves to the data, allowing the model to better align with \\nobserved trends. This can result in improved model accuracy for certain types of data.\\nLinear Regression: Less flexible, as it assumes a constant rate of change between the independent and dependent variables.\\nImproved Fit:\\n\\nPolynomial Regression: By adding polynomial terms, the model can achieve a better fit to the data, reducing residual errors \\nand potentially improving predictive performance.\\nLinear Regression: May struggle to achieve a good fit if the true relationship between variables is non-linear.\\nDisadvantages of Polynomial Regression Compared to Linear Regression\\nRisk of Overfitting:\\n\\nPolynomial Regression: With higher-degree polynomials, the model can become too complex, fitting the noise in the data rather \\nthan the underlying trend. This overfitting leads to poor generalization on new, unseen data.\\nLinear Regression: Less prone to overfitting due to its simplicity, making it more robust for generalization.\\nInterpretation Complexity:\\n\\nPolynomial Regression: The coefficients of polynomial terms are harder to interpret. The relationship between the independent\\nand dependent variables is no longer straightforward, as the effect of each variable varies depending on the value of the \\nindependent variable.\\nLinear Regression: The coefficients are easy to interpret, as each represents a constant rate of change in the dependent\\nvariable for a one-unit change in the independent variable.\\nIncreased Computational Complexity:\\n\\nPolynomial Regression: More computationally intensive due to the additional polynomial terms, which can make the model slower\\nto train and more resource-demanding, especially with very high-degree polynomials.\\nLinear Regression: Faster and more efficient to compute, as it involves fewer terms and simpler calculations.\\nExtrapolation Issues:\\n\\nPolynomial Regression: The model can produce unrealistic predictions outside the range of the observed data (extrapolation), \\nespecially with high-degree polynomials, where the curve can behave unpredictably at the boundaries.\\nLinear Regression: Typically provides more stable and reasonable predictions when extrapolating beyond the observed data range,\\nthough still with caution.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans8=\"\"\"Advantages of Polynomial Regression Compared to Linear Regression\n",
    "Ability to Model Non-Linear Relationships:\n",
    "\n",
    "Polynomial Regression: Can capture complex, non-linear relationships between the independent and dependent variables by\n",
    "incorporating higher-degree polynomial terms. This makes it suitable for datasets where the relationship is not well-represented\n",
    "by a straight line.\n",
    "Linear Regression: Limited to modeling linear relationships, which may not accurately capture the underlying patterns in \n",
    "more complex datasets.\n",
    "Flexibility:\n",
    "\n",
    "Polynomial Regression: Offers greater flexibility as it can fit curves to the data, allowing the model to better align with \n",
    "observed trends. This can result in improved model accuracy for certain types of data.\n",
    "Linear Regression: Less flexible, as it assumes a constant rate of change between the independent and dependent variables.\n",
    "Improved Fit:\n",
    "\n",
    "Polynomial Regression: By adding polynomial terms, the model can achieve a better fit to the data, reducing residual errors \n",
    "and potentially improving predictive performance.\n",
    "Linear Regression: May struggle to achieve a good fit if the true relationship between variables is non-linear.\n",
    "Disadvantages of Polynomial Regression Compared to Linear Regression\n",
    "Risk of Overfitting:\n",
    "\n",
    "Polynomial Regression: With higher-degree polynomials, the model can become too complex, fitting the noise in the data rather \n",
    "than the underlying trend. This overfitting leads to poor generalization on new, unseen data.\n",
    "Linear Regression: Less prone to overfitting due to its simplicity, making it more robust for generalization.\n",
    "Interpretation Complexity:\n",
    "\n",
    "Polynomial Regression: The coefficients of polynomial terms are harder to interpret. The relationship between the independent\n",
    "and dependent variables is no longer straightforward, as the effect of each variable varies depending on the value of the \n",
    "independent variable.\n",
    "Linear Regression: The coefficients are easy to interpret, as each represents a constant rate of change in the dependent\n",
    "variable for a one-unit change in the independent variable.\n",
    "Increased Computational Complexity:\n",
    "\n",
    "Polynomial Regression: More computationally intensive due to the additional polynomial terms, which can make the model slower\n",
    "to train and more resource-demanding, especially with very high-degree polynomials.\n",
    "Linear Regression: Faster and more efficient to compute, as it involves fewer terms and simpler calculations.\n",
    "Extrapolation Issues:\n",
    "\n",
    "Polynomial Regression: The model can produce unrealistic predictions outside the range of the observed data (extrapolation), \n",
    "especially with high-degree polynomials, where the curve can behave unpredictably at the boundaries.\n",
    "Linear Regression: Typically provides more stable and reasonable predictions when extrapolating beyond the observed data range,\n",
    "though still with caution.\"\"\"\n",
    "Ans8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01906b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
